{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import cv2\n",
        "from fer import FER\n",
        "import pyttsx3\n",
        "from keras.models import model_from_json\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "from keras.models import load_model\n",
        "from flask import Flask, render_template, Response, request\n",
        "import  tensorflow as tf\n",
        "from cvzone.HandTrackingModule import HandDetector\n",
        "from skimage.transform import resize\n",
        "model=load_model('model.h5')\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "vals=['A','B','C','D','E','F','G','H','I']\n",
        "emotion_detector = FER(mtcnn=True)\n",
        "app=Flask(__name__,template_folder=\"template\")\n",
        "print(\"Accessing video stream\")\n",
        "app.static_folder = 'static'\n",
        "vs=cv2.VideoCapture(0)\n",
        "detector=HandDetector(maxHands=1)\n",
        "pred=\"\"\n",
        "def SpeakText(command):\n",
        "    engine = pyttsx3.init()\n",
        "    engine.say(command)\n",
        "    engine.runAndWait()\n",
        "def generate_frames():\n",
        "    while (vs.isOpened()):\n",
        "        success, frame = vs.read()  \n",
        "        hands, frame=detector.findHands(frame)\n",
        "        dominant_emotion, emotion_score = emotion_detector.top_emotion(frame)\n",
        "        if not success:\n",
        "            break\n",
        "        else:\n",
        "            if hands:\n",
        "                hand=hands[0]\n",
        "                x,y,w,h=hand['bbox']\n",
        "                imgCrop=frame[y-20:y+h+20,x-20:x+w+20]\n",
        "                black=np.ones((300,300,3), np.uint8)*0\n",
        "                ishape=imgCrop.shape\n",
        "                if h/w>1:\n",
        "                    k=300/h\n",
        "                    wcal=math.ceil(k*w)\n",
        "                    imgresize=cv2.resize(imgCrop,(wcal,300))\n",
        "                    irshape=imgresize.shape\n",
        "                    wgap=math.ceil((300-wcal)/2)\n",
        "                    black[:,wgap:wcal+wgap]=imgresize\n",
        "                else:\n",
        "                    k=300/w\n",
        "                    hcal=math.ceil(k*h)\n",
        "                    imgresize=cv2.resize(imgCrop,(300,hcal))\n",
        "                    irshape=imgresize.shape\n",
        "                    hgap=math.ceil((300-hcal)/2)\n",
        "                    black[hgap:hcal+hgap,:]=imgresize\n",
        "                img=resize(black,(64,64,1))\n",
        "                img=np.expand_dims(img,axis=0)\n",
        "                if(np.max(img)>1):\n",
        "                    img = img/255.0\n",
        "                predict_x=model.predict(img)\n",
        "                classes_x=np.argmax(predict_x,axis=1)\n",
        "                x=classes_x[0]\n",
        "                SpeakText(vals[x])\n",
        "                dominant_emotion=str(dominant_emotion)\n",
        "                if(dominant_emotion!=\"\"):\n",
        "                     value=vals[x] +\" \"+ dominant_emotion\n",
        "                else:\n",
        "                    value=vals[x]\n",
        "                cv2.putText(frame,value,(x+20,y+20),cv2.FONT_HERSHEY_SIMPLEX, 1,(255, 255, 150),2,cv2.LINE_AA)\n",
        "            ret, buffer = cv2.imencode('.jpg', frame)\n",
        "            frame = buffer.tobytes()\n",
        "            yield (b'--frame\\r\\n'\n",
        "                   b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n')\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('index.html')\n",
        "@app.route('/sign_to_speech')\n",
        "def sign_to_speech():\n",
        "    return render_template('sign_to_speech.html')\n",
        "@app.route('/speech_to_sign')\n",
        "def speech_to_sign():\n",
        "    return render_template('speech_to_sign.html')\n",
        "    \n",
        "\n",
        "@app.route('/video',methods=['GET', 'POST'])\n",
        "def video():\n",
        "    return Response(generate_frames(), mimetype='multipart/x-mixed-replace; boundary=frame')\n",
        "if (__name__ == \"__main__\"):\n",
        "    app.run(debug=True)"
      ],
      "metadata": {
        "id": "ZbfMjrs1xw8x"
      }
    }
  ]
}